<!DOCTYPE html>
<!-- This site was created in Webflow. https://www.webflow.com -->
<!-- Last Published: Sun Jul 24 2022 17:45:23 GMT+0000 (Coordinated Universal Time) -->
<html data-wf-domain="mykefrs-radical-project.webflow.io" data-wf-page="6161959491c494944f593807" data-wf-site="6161959391c49406025937e1" data-wf-status="1">

<head>
    <meta charset="utf-8" />
    <title>University Project - Clinical Trials</title>
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="https://uploads-ssl.webflow.com/6161959391c49406025937e1/css/mykefrs-radical-project.webflow.f1b82102f.css" rel="stylesheet" type="text/css" />
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
</head>

<body class="body-4">
    <section id="nav-bar" class="section-4 wf-section">
        <div data-animation="default" data-collapse="medium" data-duration="400" data-easing="ease" data-easing2="ease" role="banner" class="navbar-3 w-nav">
            <div class="container-9 w-container">
                <nav role="navigation" class="nav-menu-2 w-nav-menu">
                    <div class="div-block-30">
                        <div class="div-block-31"></div><a href="https://mykefr.github.io/#Projects" class="nav-button w-button">Projects</a></div>
                    <div class="div-block-32">
                        <div class="div-block-31-copy-copy"></div><a href="https://mykefr.github.io/#skills" class="nav-button w-button">Skills</a></div>
                    <div class="div-block-32">
                        <div class="div-block-31-copy-copy-copy-2"></div><a href="https://mykefr.github.io/#education" class="nav-button w-button">Education</a></div>
                    <div class="div-block-34">
                        <div class="div-block-31 _3"></div><a href="https://mykefr.github.io/#badges" class="nav-button w-button">Badges</a></div>
                    <div class="div-block-34">
                        <div class="div-block-31 _6"></div><a href="https://mykefr.github.io/#about" class="nav-button w-button">About me</a></div>
                    <div class="div-block-34">
                        <div class="div-block-31-copy"></div><a href="https://mykefr.github.io/#contact" class="nav-button w-button">Contact</a></div>
                    <div class="div-block-33">
                        <div class="div-block-31-copy-copy-copy"></div><a href="/resume/Miguel_França_Resume.pdf" class="nav-button w-button">Resume</a></div>
                </nav>
                <div class="menu-button-2 _1 w-nav-button">
                    <div class="w-icon-nav-menu"></div>
                </div>
            </div>
        </div>
        <nav class="div-block-27 _1">
            <div class="div-block-34">
                <div class="div-block-31"></div><a href="https://mykefr.github.io/#Projects" class="nav-button _1 w-button">Projects</a></div>
            <div class="div-block-34">
                <div class="div-block-31 _1"></div><a href="https://mykefr.github.io/#skills" class="nav-button _1 w-button">Skills</a></div>
            <div class="div-block-34">
                <div class="div-block-31 _2"></div><a href="https://mykefr.github.io/#education" class="nav-button _1 w-button">Education</a></div>
            <div class="div-block-34">
                <div class="div-block-31 _3"></div><a href="https://mykefr.github.io/#badges" class="nav-button _1 w-button">Badges</a></div>
            <div class="div-block-34">
                <div class="div-block-31 _6"></div><a href="https://mykefr.github.io/#about" class="nav-button _1 w-button">About me</a></div>
            <div class="div-block-34">
                <div class="div-block-31 _5"></div><a href="https://mykefr.github.io/#contact" class="nav-button _1 w-button">Contact</a></div>
            <div class="div-block-34">
                <div class="div-block-31 _4"></div><a href="/resume/Miguel_França_Resume.pdf" class="nav-button _1 w-button">Resume</a></div>
        </nav>
    </section>
    <div class="container-11 w-container">
        <h1 class="heading-6">Matching Patient Cases to Clinical Trials with Machine Learning and Information Retrieval Models</h1>
        <h2 class="heading-7">Phase 3: Word Embeddings</h2>
        <div class="text-block-25">January 10, 2022</div><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62ded8469c852a0013249208_clinical-trials.png" loading="lazy" alt="" class="image-22" />
        <h3 class="heading-8">Index</h3><a href="#intro" class="linktab">1. Introduction</a><a href="#exp-setup" class="linktab">2. Experimental setup</a><a href="#impl" class="linktab">3. Implemented methods</a><a href="#results" class="linktab">4. Results discussion</a>
        <a
            href="#conclusion" class="linktab">5. Conclusion</a>
            <section id="intro" class="wf-section">
                <h1 class="heading-5">1. Introduction</h1>
                <div class="text-block-24 texttab _1">&emsp;The vast majority of clinical trials fail to meet their patient recruitment goal. NIH has estimated that 80% of clinical trials fail to meet their patient recruitment timeline and, more critically, many (or most) fail to recruit
                    the minimum number of patients to power the study as originally anticipated. Efficient patient trial recruitment is thus one of the major barriers to medical research, both delaying trials and forcing others to terminate entirely.<br/>‍<br/>&emsp;In
                    this project you built a system to retrieve clinical trials from <a href="https://clinicaltrials.gov/" target="_blank">ClinicalTrials.gov</a>, a required registry for clinical trials in the United States. The goal is to find clinical
                    trials where patients can be enrolled.<br/>‍<br/>&emsp;The project was divided into three phases, which allowed observing the evolution of the related technology throughout the years. <br/>‍<br/>&emsp;The first phase of the
                    project was about text analysis and matching documents to queries based on certain relevant aspects. In addition, there was a strong component on metrics to analyse the correctness of the matching made before. To do such things, we
                    had to use already given code to read the data and build a Vector Space Model based on it, as well as compute the metrics associated with the trial. Besides that, we had to<br/>implement our own Language Model, based on certain frequencies
                    of words in documents, and use it to score the different documents based on the set of test queries we had. Comparing both approaches with the metrics was essential to understand the best one for each case.<br/><br/>&emsp;The second
                    phase of the project was about learning how to better rank documents, using different sections of the clinical trial document as predictors of the clinical trial relevance for each patient case. To do so, we first had to use the algorithms
                    of the first phase to compute the predictors, scoring the documents accordingly. After that, it was necessary to combine those scores (both LMJM and VSM of different sections) and train a logistic regression to generate the best coefficients
                    to weight the different models for each section. Then we used a test set to calculate an estimate for the true quality of the prediction and calculated the metrics for the new LETOR model.<br/><br/>&emsp;Finally, the theoretical
                    study ended in techniques and algorithms of latest text analysis. The “Word Embeddings” brought an improved way of representing words, based on techniques such as “Word2Vec”, capable of establishing associations between tokens based
                    on semantics. On the other hand, “BERT” allowed to establish relationships between tokens based on the context in which these are found in a particular document.<br/>‍<br/>&emsp;This time, the focus of the third phase of the project
                    was the use of these improved algorithms to represent and visualize relationships between tokens of a given document, as well as program a model capable of classifying relevant and non-relevant documents for a certain “query”.</div>
            </section><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62ded97360b985389734603a_clinical-trials-phase2.png" loading="lazy" alt="" class="image-22" />
            <div class="text-block-26">Figure 1. The second phase of the project: The LETOR model</div>
            <section id="exp-setup" class="wf-section">
                <h1 class="heading-5">2. Experimental setup</h1>
                <div class="text-block-24 texttab _1">&emsp;For this phase of the project, we subdivided the corpus a lot, choosing only one query and two documents, only one of which was relevant to the query used. For each one of the documents, we only used the “detailed_description”
                    field, since it was enough for the analysis we proposed. We are therefore left with two corpuses, each with a special [CLS] token at the beginning and two other special [SEP] tokens, one that separates the query from the document and
                    another at the end of the corpus.<br/><br/>&emsp;Specifically for the division of the text into tokens and the extraction of their embeddings, it was necessary to use the BERT model (“Bidirectional Encoder Representations from
                    Transformers”). In addition, it was also necessary to visualize the data obtained. For all these reasons, the project was developed in python, with libraries already developed and optimized to solve these problems. Some of the most
                    important are:</div>
                <div class="text-block-24 texttwotabs _1">- transformers (transformer-type models already trained);<br/>- bertviz (view attention on models Transformer type, namely BERT);<br/>- sklearn (notably the PCA and TSNE algorithms to reduce the dimensions of the data used);<br/>- numpy;<br/>-
                    torch (framework for machine learning, namely to work with tensors, matrixes of multiple dimensions);<br/>- matplotlib (to display data images).</div>
            </section>
            <section id="impl" class="wf-section">
                <h1 class="heading-5">3. Implemented methods</h1>
                <div class="text-block-24 texttab _1">&emsp;To be able to process the text and extract the initial and contextual embeddings, we used the Transformer <a href="http://doi.org/10.1093/bioinformatics/btz682">BioBERT</a>, a biomedical language representation model designed
                    for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, question answering, etc.. This template provides ways to split text into tokens and later train and extract their embeddings by layers.
                    Thus, it is possible to extract initial embeddings accessing layer zero and final embeddings accessing the last layer (which in this model is the 13th). It is important to note that the initial layer contains the pre-trained embeddings
                    and context-independent, which does not happen with the last layer, where the relationships between the tokens in the given corpus are already considered. It is important to remember that for this work we chose two limited sets of
                    tokens (one for each corpus) in order to produce better visualizations. These were chosen so that they had a significant relationship with the corpus in question. This time, the extraction and training of embeddings occurred only for
                    the tokens included in the mentioned sets.<br/>‍<br/>&emsp;To compare the embeddings, it was necessary to resort to the mechanisms of attention and self-attention. The first one looks at the context in which each token occurs in
                    the text in order to modify its embedding, approaching tokens with a similar context. The second allows interaction between input tokens, calculating the attentions of each in relation to the others. The main focus of this work is
                    to use this model in order to make various types of visualization, listed and explained below:<br/>‍<br/>&emsp;<strong>Layer embeddings visualization:</strong><br/>&emsp;&emsp;This type of visualization is achieved by reducing
                    the number of dimensions of the embeddings to only two, using the Principal Component Analysis (PCA) algorithm. With this, a 2D scatter plot can be produced with representative points of each token, making it possible to analyze the
                    similarity of the tokens based on the distances between the points that represent them (smaller distance is equivalent to greater similarity);<br/>‍<br/>&emsp;<strong>Layer embeddings similarity visualization:</strong><br/>&emsp;&emsp;This
                    visualization presents a matrix that reproduces the similarity of each token to all the other tokens, using the cosine distance between vectors of embeddings. Warmer colors (namely yellow) represent greater similarity, while cooler
                    colors (namely purple) show less similarity. Thus, it is expected to see a diagonal with colors close to yellow, since this represents the relationship of a token to itself (which does not vary significantly). This visualization was
                    used to verify how similarities changed between the input and output of each layer;<br/>‍<br/>&emsp;<strong>Self-attention head visualization:</strong><br/>&emsp;&emsp;This view presents an interactive graph where you can
                    select certain layers and heads. In each of these, it is possible to see the connections that are established between tokens in a self-attention mechanism. The graph is especially important because it shows what connections are established
                    in each head, allowing us to infer what kind of relationship or learning took place at that stage.</div>
            </section>
            <section id="results" class="w-clearfix wf-section">
                <h1 class="heading-5">4. Results discussion</h1>
                <div class="text-block-24 texttab _1"><strong>&emsp;Layer embeddings visualization: </strong>For this visualization, we present two plots, for the tokens of the document relevant and not relevant to the chosen query:</div>
                <div class="div-block-75 _1">
                    <div class="div-block-72 _1">
                        <div class="div-block-73"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62defc3383176b883b346954_clinical-trials-scatterplot2.png" loading="lazy" alt="" class="image-23" />
                            <div class="text-block-26">Figure 2. Scatter plot for the relevant document</div>
                        </div>
                        <div class="div-block-74"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62defc250af1a85f6b92cb95_clinical-trials-scatterplot1.png" loading="lazy" alt="" class="image-24" />
                            <div class="text-block-26">Figure 3. Scatter plot for the non-relevant document</div>
                        </div>
                    </div>
                </div>
                <div class="text-block-24 texttab _1">&emsp;These plots show the points representative of the tokens selected. In red are the layer 0 points (initial embeddings) and in blue are the points of the last layer (final embeddings). Initial embeddings are only related to the
                    pre-trained word semantics , so points with similar meanings are closer to each other. We can verify that, along the layers, there was an adaptation of the embeddings, which resulted in the greater proximity of tokens that appear in
                    the same context as the corpus. This proves that the model adapts, not only to semantics, but also to the proximity of words. The graphs do not show results that are too different, despite the relevance to the query being different.
                    This happens because the model adapts to the context regardless of this factor.</div>
                <div class="text-block-24 texttab _1"><strong>&emsp;Layer embeddings similarity visualization: </strong>In this case, we show all similarity matrices between consecutive layers. Each one of them presents the comparison between the input and output embeddings of each layer:</div>
                <div
                    class="div-block-72">
                    <div class="div-block-73"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62df00c0b02feb3c94c52629_clinical-trials-sim-matrix1.png" loading="lazy" alt="" class="image-23" />
                        <div class="text-block-26">Figure 4. Similarity matrix for relevant document</div>
                    </div>
                    <div class="div-block-74"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62df00c03b03ce41dd2db33e_clinical-trials-sim-matrix2.png" loading="lazy" alt="" class="image-24" />
                        <div class="text-block-26">Figure 5. Similarity matrix for non-relevant document</div>
                    </div>
    </div>
    </section>
    <div class="text-block-24 texttab _1">&emsp;The graphs show that the embeddings change between layers, producing warmer colors if there are fewer changes. This allows us to deduce that, in the case of the relevant document, there is a greater convergence, as there are fewer and fewer
        changes between consecutive layers. Now for the non-relevant document this is verified on a smaller scale, with not as much convergence.</div>
    <div class="div-block-72">
        <div class="div-block-73"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62dfa6f24fd3c834abcd46c2_clinical-trials-sim-matrix3.png" loading="lazy" alt="" class="image-23" />
            <div class="text-block-26">Figure 6. Similarity matrix between layers 0 and 12 for the relevant document</div>
        </div>
        <div class="div-block-74"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62dfa6f2bca7086a2e4edf66_clinical-trials-sim-matrix4.png" loading="lazy" alt="" class="image-24" />
            <div class="text-block-26">Figure 7. Similarity matrix between layers 0 and 12 for the non-relevant document</div>
        </div>
    </div>
    <div class="text-block-24 texttab _1">&emsp;On the other hand, these plots show the differences between the first and the last layer, showing how much the embeddings were changed between these. Colors are inverted for better visualization, so Warmer colors correspond to greater changes.
        This makes it possible to verify that there have been major changes among these. It is also possible to verify that, for the relevant document, the most correlated with the theme of the trial and query are those that change the embeddings the
        most. This happens with the tokens “students”, “young”, “health” (the document and query are related to stress in young people). Regarding the non-relevant document, there is also an adaptation to the corpus chosen, but not related to the query
        (the embedding of “sleep”, for example, does not change too much).</div>
    <div class="text-block-24 texttab _1"><strong>&emsp;Self-attention head visualization: </strong>This view shows relationships between tokens by head and by layer. In almost all of the examples we chose the tokens in which the corpus contained a relevant document, as these were the
        examples that presented more interesting patterns. In addition, for the non-relevant document there was less links (and were also less accentuated) between words, in particular to the [CLS] token. Below are some examples of different learnt aspects:</div>
    <div
        class="div-block-72">
        <div class="div-block-73 _1"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62dfaa34938b2d371af43080_clinical-trials-heads-1.png" loading="lazy" alt="" />
            <div class="text-block-26">Figure 8. Pattern 1</div>
        </div>
        <div class="div-block-74 _1"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62dfaa354b787cb37f93fec1_clinical-trials-heads-2.png" loading="lazy" alt="" />
            <div class="text-block-26">Figure 9. Pattern 2</div>
        </div>
        <div class="div-block-73 _1"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62dfaa34f2fd5d89f873d1cb_clinical-trials-heads-3.png" loading="lazy" alt="" />
            <div class="text-block-26">Figure 10. Pattern 3</div>
        </div>
        <div class="div-block-73 _1"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62dfaa3417e3474a4e7bb2c6_clinical-trials-heads-4.png" loading="lazy" alt="" />
            <div class="text-block-26">Figure 11. Pattern 4</div>
        </div>
        </div>
        <div class="div-block-72">
            <div class="div-block-73 _1"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62dfaa347af237e86ed88087_clinical-trials-heads-5.png" loading="lazy" alt="" />
                <div class="text-block-26">Figure 12. Pattern 4</div>
            </div>
            <div class="div-block-74 _1"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62dfaa3545e33d76eccaa389_clinical-trials-heads-6.png" loading="lazy" alt="" />
                <div class="text-block-26">Figure 13. Pattern 5</div>
            </div>
            <div class="div-block-73 _1"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62dfaa34cf3563382b70b064_clinical-trials-heads-7.png" loading="lazy" alt="" />
                <div class="text-block-26">Figure 14. Pattern 1/2</div>
            </div>
            <div class="div-block-73 _1"><img src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/62dfaa33ae91248fa3281a17_clinical-trials-heads-8.png" loading="lazy" alt="" />
                <div class="text-block-26">Figure 15. Pattern 4</div>
            </div>
        </div>
        <div class="text-block-24 texttab _1"><strong>Figure 8</strong> – This specific head correlates the words with the word immediately following it (the first words chosen from this set form part of a sentence that is also in the query).<br/>‍<br/><strong>Figure 9</strong> – This example
            is similar to the previous one, but demonstrates attention to the previous word, linking them.<br/>‍<br/><strong>Figure 10</strong> – This head identifies words and expressions that are related to each other (in the same sentence), even though
            they are not completely close in the text. The sentence is: &quot;Depression, anxiety and stress are among the primary causes of disease rates worldwide and are the most prevalent mental health problems in the U.S&quot;. As we can see, the
            words “mental”, “health” and “problems” refer to “Depression ”, “anxiety” and “stress”, being this type of relationship that the head captures.<br/>‍<br/><strong>Figure 11</strong> – This example captures relationships between words in different
            sentences. In this case, a morphological relationship is presented between the words “student” and “students”.<br/>‍<br/><strong>Figure 12</strong> – This one also presents the same pattern mentioned in the previous figure, but also captures
            the semantics, linking words like “Young” and “22”, “Young” and “student”.<br/>‍<br/><strong>Figure 13</strong> – This head captures the possible prediction of words, here evidenced by the prediction of the word “problems” by the word “mental”,
            being separated from each other by the word “health”.<br/>‍<br/><strong>Figure 14</strong> – This image presents an example in which the head identifies two distinct patterns (already evidenced in the patterns in figures 7 and 8), capable
            of associating a word with either the word that precedes it or the one that precedes it.<br/>‍<br/><strong>Figure 15</strong> – This image just shows a curious relationship between a verb in the infinitive and its gerund, linking “sleep” to
            “sleeping”. This case is from a non-relevant document, although this has no influence on how the relationship is established between the words in the document.</div>
        <section id="conclusion" class="wf-section">
            <h1 class="heading-5">5. Conclusion</h1>
            <div class="text-block-24 texttab _1">&emsp;This project brought the possibility of applying and putting into practice the theoretical knowledge studied in class. Facing the real difficulties of programming, using or analyzing the results of information retrieval algorithms
                made us learn more about them and realize their true usefulness in real issues, such as the attribution of patient cases to clinical trials. In addition, seeing the evolution of technology was very interesting, going through simple models
                based on the frequency of words/tokens in the corpus and others capable of better understanding the semantic similarities and the correlation between tokens looking at the context in which they are inserted.<br/>&emsp;This phase concerns
                the last mentioned models, which turn out to be more robust and provide a better analysis of the text. The word2vec and BERT models are pre-trained with a large set of documents, which makes them very good at understanding the semantic
                similarity between words. In addition, BERT allows a more personalized training, using multiple layers and heads to learn various correlations between words, based on the context in which they occur in the corpus of data provided.<br/>&emsp;With
                that being said, we believe that analyzing the data has helped us understand how algorithms work, what they are capable of doing, and how we could use them on a personal or professional level to improve an information retrieval activity.</div>
            <div
                id="spacer" class="div-block-76"></div>
        </section>
        </div>
        <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=6161959391c49406025937e1" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
        <script src="https://uploads-ssl.webflow.com/6161959391c49406025937e1/js/webflow.cfae578f0.js" type="text/javascript"></script>
        <!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->
    <script src="../einm2imeim392me82em28s82s.js"></script>  
</body>

</html>